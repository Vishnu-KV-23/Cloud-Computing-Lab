from pyspark import SparkContext

# Initialize SparkContext

sc = SparkContext("local","palindrome")

# Load the input data

text_file = sc.textFile("path/to/your/textfile.txt")




#same for all programs in pyspark until now







from pyspark import SparkContext
sc=SparkContext('local','wordcount')
txt=sc.textFile('/home/vyshnavi/Documents/vys.txt')

val=txt.flatMap(lambda line:line.split(","))

valm=val.map(lambda char:(char,1))
ls=valm.filter(lambda s:s[0]==s[0][::-1])
red=ls.reduceByKey(lambda a,b:a+b)

resu=red.collect()
for char,count in resu:
print(f"palindromes\n{char}={count}")

















































#flaptmap- to flatten text
#to sep using space and all


#map-


# Function to check if a word is a palindrome
def is_palindrome(word):
    return word == word[::-1]

# Find palindromic words
palindromic_words = (text_file
                     .flatMap(lambda line: line.split())      # Split each line into words
                     .filter(is_palindrome)                   # Filter to keep only palindromic words
                     .distinct())                             # Remove duplicates

# Collect and print the results
for word in palindromic_words.collect():
    print(word)

# Stop the SparkContext
sc.stop()
